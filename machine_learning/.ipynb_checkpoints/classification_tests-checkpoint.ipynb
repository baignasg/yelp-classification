{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Display the top features in each LDA category\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in the H&L dictionaries\n",
    "negative_words = open('../input/negative-words.txt', 'r').read()\n",
    "negative_words = negative_words.split('\\n')\n",
    "positive_words = open('../input/positive-words.txt', 'r').read()\n",
    "positive_words = positive_words.split('\\n')\n",
    "total_words = negative_words + positive_words\n",
    "total_words = list(set(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in the subsetted JSON data and then create a dataframe \n",
    "#with the reviews and stars as separate columns\n",
    "review_data = json.load(open('../input/cleaned_reviews_subset.json'))\n",
    "reviews = []\n",
    "stars = []\n",
    "for state in review_data.keys():\n",
    "    for review in review_data[state]:\n",
    "        reviews.append(review['text'])\n",
    "        if review['stars'] >= 4:\n",
    "            stars.append(review['stars'])\n",
    "        else:\n",
    "            stars.append(1)\n",
    "cleaned_data = pd.DataFrame({'review_text': reviews, 'review_stars': stars})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Learn a 200-topic LDA representation using the full corpus\n",
    "vectorizer = CountVectorizer(stop_words='english', vocabulary = total_words )\n",
    "tf = vectorizer.fit_transform(cleaned_data['review_text'])\n",
    "lda_fit = LDA(n_topics=100, learning_offset = 1.5).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "nice happy tender smile disappoint impeccable thrilled easier appropriate generously\n",
      "Topic 1:\n",
      "cold fine like poor disgusting stuck bad hype noise overwhelming\n",
      "Topic 2:\n",
      "hot fresh like delicious ready liked seasoned die sour incredible\n",
      "Topic 3:\n",
      "friendly favorite super clean great fresh fantastic beautiful greasy good\n",
      "Topic 4:\n",
      "anxiously well well-established praise imaginative misaligns smallish cleanly expropriation dissolute\n",
      "Topic 5:\n",
      "horrible awful like strong smell sadly smelled ruined complained sorry\n",
      "Topic 6:\n",
      "great concede revere negation inadequate treacherously hectic cranky curt profound\n",
      "Topic 7:\n",
      "juddering perturb stall muddle well-positioned improving conflicted assault avidly droops\n",
      "Topic 8:\n",
      "odd correct lacked impressive smoke annoyed compliment hassle complementary ideal\n",
      "Topic 9:\n",
      "world-famous inappropriately cuss dissident condemn shortage scornful eccentricity villainous aborted\n",
      "Topic 10:\n",
      "lacking bumped suffers disbelief master originality good pretty favorite hard\n",
      "Topic 11:\n",
      "tepid smoke unwavering prodigy rumors heretical corrupts nervously vengefully regrets\n",
      "Topic 12:\n",
      "neglected accurately curse good worth expensive bugging madden kills malady\n",
      "Topic 13:\n",
      "like good bad pretty warm problem pricey disappointing lemon healthy\n",
      "Topic 14:\n",
      "cracked pinch cruel disapointing illogically ineffectually undesirable accomplishment discontentedly dishonesty\n",
      "Topic 15:\n",
      "better right like solid soft tough consistent messy skeptical scrambled\n",
      "Topic 16:\n",
      "love recommend great favorite perfection wonder fell stellar eager respect\n",
      "Topic 17:\n",
      "charm satisfactory apprehensive bumped excellent wow right master stink spectacular\n",
      "Topic 18:\n",
      "disappointed loud issue popular joke hung broken winner amazingly fans\n",
      "Topic 19:\n",
      "cheerful manageable winners inflame correct sensation pep beautifully disintegrated stupidly\n",
      "Topic 20:\n",
      "worst sorry break appreciate issues bad plentiful concerned tired nicer\n",
      "Topic 21:\n",
      "mature barbarian excellent nice great right love decent awesome clean\n",
      "Topic 22:\n",
      "awesome great smells fail insane shrug limit unreal frigging unlimited\n",
      "Topic 23:\n",
      "noiseless subsidize hogs backbiting insulted irritably racists delude goad crush\n",
      "Topic 24:\n",
      "outsmart irritating ecstasies jutters flagrant picky apprehensions pleasantly cancerous riled\n",
      "Topic 25:\n",
      "harried amazing good lacking easiest foreboding feisty manipulators praising sadness\n",
      "Topic 26:\n",
      "illegally brashly exaggeration fallout stumbles revives renunciation excitingly goad excites\n",
      "Topic 27:\n",
      "fabulous complimentary shame jam fear boiling ease brat treasure anxious\n",
      "Topic 28:\n",
      "fast blame lame frustrating patience unappealing fantastically friendly super delicious\n",
      "Topic 29:\n",
      "right horrible scary breaking effusive captivate perfunctory high-quality farcically boundless\n",
      "Topic 30:\n",
      "confusing distracting negatives truthfully subtract unlimited yay satisfies delectable miracle\n",
      "Topic 31:\n",
      "excitement slick tremendously preferable succeeded intolerable amazing lacking good bore\n",
      "Topic 32:\n",
      "overwhelmingly grace fibber rail uneconomical tough rocky efficacious sever disarm\n",
      "Topic 33:\n",
      "best expensive mediocre fancy fresh falls modern support warned ample\n",
      "Topic 34:\n",
      "difficult inventive screwed risk disorganized deter frustration improvements struggle fortunate\n",
      "Topic 35:\n",
      "upscale classy dissapointed chaos generosity posh mistaken unexpected craps unaffordable\n",
      "Topic 36:\n",
      "enjoy wrong free desert blah bothered pain refused amazed miserable\n",
      "Topic 37:\n",
      "cons pros floored reassurance stylized plight originality insecurity smother luxurious\n",
      "Topic 38:\n",
      "fun cute lovely stale dead knock helping grace pricier disaster\n",
      "Topic 39:\n",
      "buzzing offensiveness warm sin lifeless undermining deceit gimmicked zaps gimmick\n",
      "Topic 40:\n",
      "great excellent toughest harm thrilled sickening oppression unrelentingly smash derision\n",
      "Topic 41:\n",
      "impressed rude fair prompt mistake downside safe celebrate unacceptable hated\n",
      "Topic 42:\n",
      "extremists wince deceiving paucity glitz wows infidel despondently recommended assure\n",
      "Topic 43:\n",
      "negative incorrect rocky good annoyance fresh recommend tank scarcity distraction\n",
      "Topic 44:\n",
      "freezing absurdly lapse funky cocky wince examplary delinquent scarcity effusion\n",
      "Topic 45:\n",
      "commonplace good recommend unnerving annoyances hard-line happiness crashing excruciating unfeeling\n",
      "Topic 46:\n",
      "crowded cozy crazy satisfied bloody tops fool lover attractive chic\n",
      "Topic 47:\n",
      "welcome famous ugh accessible oddly fond competitive zombie sensible poignant\n",
      "Topic 48:\n",
      "biased aching gained abscond agreeableness revel smudged unjustly tenaciously smudge\n",
      "Topic 49:\n",
      "good pretty great generous variety hang fresh prefer funny heaven\n",
      "Topic 50:\n",
      "worries fame din feverish worried defile affection harden stresses jeer\n",
      "Topic 51:\n",
      "great delicious perfect wonderful friendly fantastic variety thank perfectly mashed\n",
      "Topic 52:\n",
      "sweet cheap wow recommendations cheaper lazy exciting overdone satisfy dislike\n",
      "Topic 53:\n",
      "pretend preeminent hardships irritate horrified wins deplete trauma irrecoverably illness\n",
      "Topic 54:\n",
      "garbage smiling capable impeccably disturbance exorbitant vain eschew knave insignificance\n",
      "Topic 55:\n",
      "recommended memorable punch fortunately killed lousy lead robust nonsense absurd\n",
      "Topic 56:\n",
      "comfortable phenomenal spacious sucker coolest precious conservative great swanky lacked\n",
      "Topic 57:\n",
      "bent fantastic better elegant nauseates mendacity lewd greed subjugation blasted\n",
      "Topic 58:\n",
      "decent unfortunately nasty pig beware golden delay attack limp tank\n",
      "Topic 59:\n",
      "confusion like sweet split favorite decent amazing feat resigned flicker\n",
      "Topic 60:\n",
      "gem yay spotty snagged dependable reasonable delight happy delicious best\n",
      "Topic 61:\n",
      "sufficient ding fanfare beg adamantly polluter astonished damnably dirt-cheap quibbles\n",
      "Topic 62:\n",
      "weird works gross trendy poorly skinny fave unbelievable insult leading\n",
      "Topic 63:\n",
      "shockingly infected hoard crap congested deceiver favored erratic inexorable unwanted\n",
      "Topic 64:\n",
      "bullshit chronic uncertain great clean awesome decent dawn graceless fallaciousness\n",
      "Topic 65:\n",
      "won outstanding polite creative clear knowledgeable lie lively blow sketchy\n",
      "Topic 66:\n",
      "sad squash funky crappy luster dump doomed homage right best\n",
      "Topic 67:\n",
      "quiet noisy excuse worry dust novelty smelling baffled gooood enticed\n",
      "Topic 68:\n",
      "enjoyed cool interesting sick sucks twist sparkling hurting stink tantrum\n",
      "Topic 69:\n",
      "hard reasonable pleasant reasonably better delightful smart extraordinary kudos unpleasant\n",
      "Topic 70:\n",
      "strangely win modest sharper payback bruised purposeful audible adamantly useless\n",
      "Topic 71:\n",
      "suffered entertain errors dent unproven deluded revival gross mispronounced adulterier\n",
      "Topic 72:\n",
      "amazing loved great split available perfect crisp like worked fat\n",
      "Topic 73:\n",
      "knife amazing great measly audibly swanky rightly immovable skill subversion\n",
      "Topic 74:\n",
      "glad loves weak finest silly embarrassing cheer lied panicked gritty\n",
      "Topic 75:\n",
      "quaint cheat adore good amazing wonderful clean damage detracts outraged\n",
      "Topic 76:\n",
      "great good deservedly awesome cheap durable drips beutifully degradingly arduously\n",
      "Topic 77:\n",
      "worth pleased fairly lack hate classic lost cheesy convenient prefer\n",
      "Topic 78:\n",
      "hero disgust courageous perfect busts malign fortitude bored terribly forlornly\n",
      "Topic 79:\n",
      "terrible excited waste favor upset mistakes angry upbeat hates scare\n",
      "Topic 80:\n",
      "biting fatuously refusal hater oversimplified fallout doubts innocuous contradiction soothingly\n",
      "Topic 81:\n",
      "slow helpful helped chill stumbled dim cleared unbelievably pride flair\n",
      "Topic 82:\n",
      "attentive spectacular complaints romantic snob complemented bs favorite great delicious\n",
      "Topic 83:\n",
      "authentic positive missed honest impress drunk delectable unfriendly positives dying\n",
      "Topic 84:\n",
      "limited superior complaining hefty intrusive nitpick good like great tank\n",
      "Topic 85:\n",
      "fried like easy perfectly miss rich consistently exceptional bother needless\n",
      "Topic 86:\n",
      "legendary love fantastic complimentary favorite ease tyrannical overjoyed exhilaration spew\n",
      "Topic 87:\n",
      "promptly comfort picky criticism guilty patiently shiny monstrous miserably ache\n",
      "Topic 88:\n",
      "lyrical gimmicking enliven disliked backaching unwelcome wickedly treacherous euphoric adroitly\n",
      "Topic 89:\n",
      "mighty fondness gained aching dignify brilliant cheaply fanaticism scandels motivated\n",
      "Topic 90:\n",
      "innovative dazed bullshit great cheap awesome deservedly good sumptuously infuriating\n",
      "Topic 91:\n",
      "fatty suck lean unable reluctantly great sorry impressed rip poignant\n",
      "Topic 92:\n",
      "overrated tidy repulsive incompatability smallish deception disgusting ultimatums demon saint\n",
      "Topic 93:\n",
      "struggles dreadful sustainable bewilderment slur fervidly loathe blockbuster vengeful graciousness\n",
      "Topic 94:\n",
      "excellent bland complaint frozen fresh balanced grand spoiled celebration thicker\n",
      "Topic 95:\n",
      "recover enchant dedicated miseries vainly crude ragged liberation desolation jaundiced\n",
      "Topic 96:\n",
      "work overpriced shit bored allergy passion enthusiastic skimpy warmth struck\n",
      "Topic 97:\n",
      "warmly crime vague victory loose anger squirm warily affirmation beneficiary\n",
      "Topic 98:\n",
      "bomb nicest straightforward gooood loser tingled disgraced fascination losers extort\n",
      "Topic 99:\n",
      "likes excuses clueless stronger dick lonely bad right neglect like\n"
     ]
    }
   ],
   "source": [
    "#View the top 10 words in the LDA representation\n",
    "no_top_words = 10\n",
    "tf_feature_names = vectorizer.get_feature_names()\n",
    "display_topics(lda_fit, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transform our reviews into LDA representations\n",
    "lda_features = vectorizer.transform(cleaned_data['review_text'])\n",
    "lda_features = lda_fit.transform(lda_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.76      0.71       378\n",
      "          4       0.46      0.23      0.31       282\n",
      "          5       0.59      0.73      0.65       341\n",
      "\n",
      "avg / total       0.58      0.60      0.58      1001\n",
      "\n",
      "[[289  43  46]\n",
      " [ 86  66 130]\n",
      " [ 59  33 249]]\n"
     ]
    }
   ],
   "source": [
    "#Do a baseline test using linear SVM, print classification report\n",
    "#Train, test, split\n",
    "X_train, X_test, y_train, y_test = tts(lda_features, cleaned_data['review_stars'], test_size=0.2)\n",
    "#Create a decision tree classifier object\n",
    "svm_clf = svm.LinearSVC()\n",
    "#kernel='sigmoid'\n",
    "#Train the Decision Forest Classifier\n",
    "svm_clf.fit(X_train, y_train)\n",
    "#Predict on the test set\n",
    "test_prediction = svm_clf.predict(X_test)\n",
    "#Print a classification report\n",
    "print classification_report(y_test, test_prediction)\n",
    "print confusion_matrix(y_test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.60      0.92      0.72       375\n",
      "          4       0.46      0.12      0.19       279\n",
      "          5       0.64      0.65      0.64       347\n",
      "\n",
      "avg / total       0.57      0.60      0.55      1001\n",
      "\n",
      "[[344  11  20]\n",
      " [137  33 109]\n",
      " [ 93  28 226]]\n"
     ]
    }
   ],
   "source": [
    "#Train a tf-idf vector using the H&L dictionary\n",
    "tf_vectorizer = TfidfVectorizer(ngram_range = (3,3))\n",
    "tf_features = tf_vectorizer.fit_transform(cleaned_data['review_text'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_features, cleaned_data['review_stars'], test_size=0.2)\n",
    "#Create a decision tree classifier object\n",
    "tf_classifier = svm.LinearSVC()\n",
    "#Train the Decision Forest Classifier\n",
    "tf_classifier.fit(X_train, y_train)\n",
    "#Predict on the test set\n",
    "test_prediction = tf_classifier.predict(X_test)\n",
    "#Print a classification report\n",
    "print classification_report(y_test, test_prediction)\n",
    "print confusion_matrix(y_test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertsonwang/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Create a combined dictionary of the features from LDA and tf-idf\n",
    "tf_idf_mod = TfidfVectorizer(ngram_range = (3,3))\n",
    "lda_mod = LDA(n_topics=200, learning_offset = 1.5)\n",
    "combined_features = FeatureUnion([\n",
    "         ('lda', Pipeline([\n",
    "      ('counts', CountVectorizer(stop_words='english', vocabulary = total_words)),\n",
    "      ('tf_idf', lda_mod)\n",
    "    ])),\n",
    "    ('tf_idf', tf_idf_mod)])\n",
    "comb_features = combined_features.fit(cleaned_data['review_text']).transform(cleaned_data['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Export the pickled version of this classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.77      0.88      0.82      2506\n",
      "          4       0.73      0.47      0.57      2027\n",
      "          5       0.72      0.82      0.77      2469\n",
      "\n",
      "avg / total       0.74      0.74      0.73      7002\n",
      "\n",
      "[[2204  150  152]\n",
      " [ 417  956  654]\n",
      " [ 226  210 2033]]\n"
     ]
    }
   ],
   "source": [
    "#We can do better then simply looking at the tf-idf or LDA matrices on their own\n",
    "#Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(comb_features, cleaned_data['review_stars'], test_size=0.2)\n",
    "#Create a decision tree classifier object\n",
    "comb_clf = svm.LinearSVC()\n",
    "#Train the Classifier\n",
    "comb_clf.fit(X_train, y_train)\n",
    "#Predict on the test set\n",
    "test_prediction = comb_clf.predict(X_test)\n",
    "#Print a classification report\n",
    "print classification_report(y_test, test_prediction)\n",
    "print confusion_matrix(y_test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          0.81727803  0.65752485  0.64929722\n",
      "  0.62495715  0.62461433  0.65923894  0.66815221  0.66918067  0.68015084]\n"
     ]
    }
   ],
   "source": [
    "#Print the cross validated scores\n",
    "scores = cross_val_score(svm.LinearSVC(), comb_features, cleaned_data['review_stars'], cv=12)\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.30)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
